{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf3d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\pawne\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\pawne\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pawne\\anaconda3\\lib\\site-packages (from pytest) (23.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\pawne\\anaconda3\\lib\\site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pawne\\anaconda3\\lib\\site-packages (from pytest) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7260ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
      "\n",
      "positional arguments:\n",
      "  file_or_dir\n",
      "\n",
      "general:\n",
      "  -k EXPRESSION         Only run tests which match the given substring\n",
      "                        expression. An expression is a Python evaluatable\n",
      "                        expression where all names are substring-matched against\n",
      "                        test names and their parent classes. Example: -k\n",
      "                        'test_method or test_other' matches all test functions\n",
      "                        and classes whose name contains 'test_method' or\n",
      "                        'test_other', while -k 'not test_method' matches those\n",
      "                        that don't contain 'test_method' in their names. -k 'not\n",
      "                        test_method and not test_other' will eliminate the\n",
      "                        matches. Additionally keywords are matched to classes\n",
      "                        and functions containing extra names in their\n",
      "                        'extra_keyword_matches' set, as well as functions which\n",
      "                        have names assigned directly to them. The matching is\n",
      "                        case-insensitive.\n",
      "  -m MARKEXPR           Only run tests matching given mark expression. For\n",
      "                        example: -m 'mark1 and not mark2'.\n",
      "  --markers             show markers (builtin, plugin and per-project ones).\n",
      "  -x, --exitfirst       Exit instantly on first error or failed test\n",
      "  --fixtures, --funcargs\n",
      "                        Show available fixtures, sorted by plugin appearance\n",
      "                        (fixtures with leading '_' are only shown with '-v')\n",
      "  --fixtures-per-test   Show fixtures per test\n",
      "  --pdb                 Start the interactive Python debugger on errors or\n",
      "                        KeyboardInterrupt\n",
      "  --pdbcls=modulename:classname\n",
      "                        Specify a custom interactive Python debugger for use\n",
      "                        with --pdb.For example:\n",
      "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
      "  --trace               Immediately break when running each test\n",
      "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
      "  -s                    Shortcut for --capture=no\n",
      "  --runxfail            Report the results of xfail tests as if they were not\n",
      "                        marked\n",
      "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all\n",
      "                        if none failed)\n",
      "  --ff, --failed-first  Run all tests, but run the last failures first. This may\n",
      "                        re-order tests and thus lead to repeated fixture\n",
      "                        setup/teardown.\n",
      "  --nf, --new-first     Run tests from new files first, then the rest of the\n",
      "                        tests sorted by file mtime\n",
      "  --cache-show=[CACHESHOW]\n",
      "                        Show cache contents, don't perform collection or tests.\n",
      "                        Optional argument: glob (default: '*').\n",
      "  --cache-clear         Remove all cache contents at start of test run\n",
      "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
      "                        Which tests to run with no previously (known) failures\n",
      "  --sw, --stepwise      Exit on test failure and continue from last failing test\n",
      "                        next time\n",
      "  --sw-skip, --stepwise-skip\n",
      "                        Ignore the first failing test but stop on the next\n",
      "                        failing test. Implicitly enables --stepwise.\n",
      "\n",
      "Reporting:\n",
      "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
      "  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n",
      "                        list. Default: 0.005.\n",
      "  -v, --verbose         Increase verbosity\n",
      "  --no-header           Disable header\n",
      "  --no-summary          Disable summary\n",
      "  -q, --quiet           Decrease verbosity\n",
      "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
      "  -r chars              Show extra test summary info as specified by chars:\n",
      "                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n",
      "                        (p)assed, (P)assed with output, (a)ll except passed\n",
      "                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
      "                        --disable-warnings), 'N' can be used to reset the list.\n",
      "                        (default: 'fE').\n",
      "  --disable-warnings, --disable-pytest-warnings\n",
      "                        Disable warnings summary\n",
      "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
      "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed\n",
      "                        through addopts)\n",
      "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
      "  --show-capture={no,stdout,stderr,log,all}\n",
      "                        Controls how captured stdout/stderr/log is shown on\n",
      "                        failed tests. Default: all.\n",
      "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
      "  --color=color         Color terminal output (yes/no/auto)\n",
      "  --code-highlight={yes,no}\n",
      "                        Whether code should be highlighted (only if --color is\n",
      "                        also enabled). Default: yes.\n",
      "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
      "  --junit-xml=path      Create junit-xml style report file at given path\n",
      "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
      "\n",
      "pytest-warnings:\n",
      "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
      "                        Set which warnings to report, see -W option of Python\n",
      "                        itself\n",
      "  --maxfail=num         Exit after first num failures or errors\n",
      "  --strict-config       Any warnings encountered while parsing the `pytest`\n",
      "                        section of the configuration file raise errors\n",
      "  --strict-markers      Markers not registered in the `markers` section of the\n",
      "                        configuration file raise errors\n",
      "  --strict              (Deprecated) alias to --strict-markers\n",
      "  -c FILE, --config-file=FILE\n",
      "                        Load configuration from `FILE` instead of trying to\n",
      "                        locate one of the implicit configuration files.\n",
      "  --continue-on-collection-errors\n",
      "                        Force test execution even if collection errors occur\n",
      "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
      "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
      "                        absolute path: '/home/user/root_dir'; path with\n",
      "                        variables: '$HOME/root_dir'.\n",
      "\n",
      "collection:\n",
      "  --collect-only, --co  Only collect tests, don't execute them\n",
      "  --pyargs              Try to interpret all arguments as Python packages\n",
      "  --ignore=path         Ignore path during collection (multi-allowed)\n",
      "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
      "  --deselect=nodeid_prefix\n",
      "                        Deselect item (via node id prefix) during collection\n",
      "                        (multi-allowed)\n",
      "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
      "  --noconftest          Don't load any conftest.py files\n",
      "  --keep-duplicates     Keep duplicate tests\n",
      "  --collect-in-virtualenv\n",
      "                        Don't ignore tests in a local virtualenv directory\n",
      "  --import-mode={prepend,append,importlib}\n",
      "                        Prepend/append to sys.path when importing test modules\n",
      "                        and conftest files. Default: prepend.\n",
      "  --doctest-modules     Run doctests in all .py modules\n",
      "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
      "                        Choose another output format for diffs on doctest\n",
      "                        failure\n",
      "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
      "  --doctest-ignore-import-errors\n",
      "                        Ignore doctest ImportErrors\n",
      "  --doctest-continue-on-failure\n",
      "                        For a given doctest, continue to run after the first\n",
      "                        failure\n",
      "\n",
      "test session debugging and configuration:\n",
      "  --basetemp=dir        Base temporary directory for this test run. (Warning:\n",
      "                        this directory is removed if it exists.)\n",
      "  -V, --version         Display pytest version and information about plugins.\n",
      "                        When given twice, also display information about\n",
      "                        plugins.\n",
      "  -h, --help            Show help message and configuration info\n",
      "  -p name               Early-load given plugin module name or entry point\n",
      "                        (multi-allowed). To avoid loading of plugins, use the\n",
      "                        `no:` prefix, e.g. `no:doctest`.\n",
      "  --trace-config        Trace considerations of conftest.py files\n",
      "  --debug=[DEBUG_FILE_NAME]\n",
      "                        Store internal tracing debug information in this log\n",
      "                        file. This file is opened with 'w' and truncated as a\n",
      "                        result, care advised. Default: pytestdebug.log.\n",
      "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
      "                        Override ini option with \"option=value\" style, e.g. `-o\n",
      "                        xfail_strict=True -o cache_dir=cache`.\n",
      "  --assert=MODE         Control assertion debugging tools.\n",
      "                        'plain' performs no assertion debugging.\n",
      "                        'rewrite' (the default) rewrites assert statements in\n",
      "                        test modules on import to provide assert expression\n",
      "                        information.\n",
      "  --setup-only          Only setup fixtures, do not execute tests\n",
      "  --setup-show          Show setup of fixtures while executing tests\n",
      "  --setup-plan          Show what fixtures and tests would be executed but don't\n",
      "                        execute anything\n",
      "\n",
      "logging:\n",
      "  --log-level=LEVEL     Level of messages to catch/display. Not set by default,\n",
      "                        so it depends on the root/parent log handler's effective\n",
      "                        level, where it is \"WARNING\" by default.\n",
      "  --log-format=LOG_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-date-format=LOG_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-cli-level=LOG_CLI_LEVEL\n",
      "                        CLI logging level\n",
      "  --log-cli-format=LOG_CLI_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
      "  --log-file-level=LOG_FILE_LEVEL\n",
      "                        Log file logging level\n",
      "  --log-file-format=LOG_FILE_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-auto-indent=LOG_AUTO_INDENT\n",
      "                        Auto-indent multiline messages passed to the logging\n",
      "                        module. Accepts true|on, false|off or an integer.\n",
      "  --log-disable=LOGGER_DISABLE\n",
      "                        Disable a logger by name. Can be passed multiple times.\n",
      "\n",
      "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
      "\n",
      "  markers (linelist):   Markers for test functions\n",
      "  empty_parameter_set_mark (string):\n",
      "                        Default marker for empty parametersets\n",
      "  norecursedirs (args): Directory patterns to avoid for recursion\n",
      "  testpaths (args):     Directories to search for tests when no files or\n",
      "                        directories are given on the command line\n",
      "  filterwarnings (linelist):\n",
      "                        Each line specifies a pattern for\n",
      "                        warnings.filterwarnings. Processed after\n",
      "                        -W/--pythonwarnings.\n",
      "  usefixtures (args):   List of default fixtures to be used with this project\n",
      "  python_files (args):  Glob-style file patterns for Python test module\n",
      "                        discovery\n",
      "  python_classes (args):\n",
      "                        Prefixes or glob names for Python test class discovery\n",
      "  python_functions (args):\n",
      "                        Prefixes or glob names for Python test function and\n",
      "                        method discovery\n",
      "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
      "                        Disable string escape non-ASCII characters, might cause\n",
      "                        unwanted side effects(use at your own risk)\n",
      "  console_output_style (string):\n",
      "                        Console output: \"classic\", or with additional progress\n",
      "                        information (\"progress\" (percentage) | \"count\" |\n",
      "                        \"progress-even-when-capture-no\" (forces progress even\n",
      "                        when capture=no)\n",
      "  xfail_strict (bool):  Default for the strict parameter of xfail markers when\n",
      "                        not given explicitly (default: False)\n",
      "  tmp_path_retention_count (string):\n",
      "                        How many sessions should we keep the `tmp_path`\n",
      "                        directories, according to `tmp_path_retention_policy`.\n",
      "  tmp_path_retention_policy (string):\n",
      "                        Controls which directories created by the `tmp_path`\n",
      "                        fixture are kept around, based on test outcome.\n",
      "                        (all/failed/none)\n",
      "  enable_assertion_pass_hook (bool):\n",
      "                        Enables the pytest_assertion_pass hook. Make sure to\n",
      "                        delete any previously generated pyc cache files.\n",
      "  junit_suite_name (string):\n",
      "                        Test suite name for JUnit report\n",
      "  junit_logging (string):\n",
      "                        Write captured log messages to JUnit report: one of\n",
      "                        no|log|system-out|system-err|out-err|all\n",
      "  junit_log_passing_tests (bool):\n",
      "                        Capture log information for passing tests to JUnit\n",
      "                        report:\n",
      "  junit_duration_report (string):\n",
      "                        Duration time to report: one of total|call\n",
      "  junit_family (string):\n",
      "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
      "  doctest_optionflags (args):\n",
      "                        Option flags for doctests\n",
      "  doctest_encoding (string):\n",
      "                        Encoding used for doctest files\n",
      "  cache_dir (string):   Cache directory path\n",
      "  log_level (string):   Default value for --log-level\n",
      "  log_format (string):  Default value for --log-format\n",
      "  log_date_format (string):\n",
      "                        Default value for --log-date-format\n",
      "  log_cli (bool):       Enable log display during test run (also known as \"live\n",
      "                        logging\")\n",
      "  log_cli_level (string):\n",
      "                        Default value for --log-cli-level\n",
      "  log_cli_format (string):\n",
      "                        Default value for --log-cli-format\n",
      "  log_cli_date_format (string):\n",
      "                        Default value for --log-cli-date-format\n",
      "  log_file (string):    Default value for --log-file\n",
      "  log_file_level (string):\n",
      "                        Default value for --log-file-level\n",
      "  log_file_format (string):\n",
      "                        Default value for --log-file-format\n",
      "  log_file_date_format (string):\n",
      "                        Default value for --log-file-date-format\n",
      "  log_auto_indent (string):\n",
      "                        Default value for --log-auto-indent\n",
      "  pythonpath (paths):   Add paths to sys.path\n",
      "  faulthandler_timeout (string):\n",
      "                        Dump the traceback of all threads if a test takes more\n",
      "                        than TIMEOUT seconds to finish\n",
      "  addopts (args):       Extra command line options\n",
      "  minversion (string):  Minimally required pytest version\n",
      "  required_plugins (args):\n",
      "                        Plugins that must be present for pytest to run\n",
      "\n",
      "Environment variables:\n",
      "  PYTEST_ADDOPTS           Extra command line options\n",
      "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
      "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
      "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
      "\n",
      "\n",
      "to see available markers type: pytest --markers\n",
      "to see available fixtures type: pytest --fixtures\n",
      "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
     ]
    }
   ],
   "source": [
    "!pytest -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221a3a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_sample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sample.py\n",
    "import pytest\n",
    "def test_file1_method():\n",
    "    x=5\n",
    "    y=6\n",
    "    assert x+1==y, \"test failed\"\n",
    "    assert x==y, \"test failed\"\n",
    "def test_file2_method():\n",
    "    x=5\n",
    "    y=6\n",
    "    assert x+1==y, \"test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae67e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.0.0\n",
      "rootdir: C:\\Users\\pawne\\lab\n",
      "plugins: anyio-3.5.0\n",
      "collected 2 items\n",
      "\n",
      "test_sample.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_file1_method ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_file1_method\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        x=\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        y=\u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m x+\u001b[94m1\u001b[39;49;00m==y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m x==y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: test failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 6\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_sample.py::\u001b[1mtest_file1_method\u001b[0m - AssertionError: test failed\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.20s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80de09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.0.0\n",
      "rootdir: C:\\Users\\pawne\\lab\n",
      "plugins: anyio-3.5.0\n",
      "collected 2 items\n",
      "\n",
      "test_sample.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_file1_method ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_file1_method\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        x=\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        y=\u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m x+\u001b[94m1\u001b[39;49;00m==y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m x==y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: test failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 6\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_sample.py::\u001b[1mtest_file1_method\u001b[0m - AssertionError: test failed\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.39s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424c4f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.0.0 -- C:\\Users\\pawne\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\pawne\\lab\n",
      "plugins: anyio-3.5.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: file or directory not found: #\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test -k test_file1_method -v # 세세한 수식어는 외우지 말자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a207292e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3047414481.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    py.test -m test_sample.py\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "py.test -m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93c73993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_fixture.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa=25\n",
    "    bb=35\n",
    "    cc=45\n",
    "    return [aa,bb,cc]\n",
    "\n",
    "def test_compareWithAA(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[0]==zz, \"aa and zz comparison failed\"\n",
    "    \n",
    "def test_compareWithBB(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[1]==zz, \"aa and zz comparison failed\"\n",
    "\n",
    "def test_compareWithCC(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[2]==zz, \"aa and zz comparison failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3a42e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function\n"
     ]
    }
   ],
   "source": [
    "def my_func(func, x):\n",
    "    print(\"function\")\n",
    "    func(x)\n",
    "    \n",
    "def cs_func(x):\n",
    "    return x**x\n",
    "my_func(cs_func, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fbd83f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.0.0\n",
      "rootdir: C:\\Users\\pawne\\lab\n",
      "plugins: anyio-3.5.0\n",
      "collected 3 items\n",
      "\n",
      "test_fixture.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                      [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________________ test_compareWithAA ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compareWithAA\u001b[39;49;00m(supply_AA_BB_CC):\u001b[90m\u001b[39;49;00m\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m0\u001b[39;49;00m]==zz, \u001b[33m\"\u001b[39;49;00m\u001b[33maa and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: aa and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 25 == 35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixture.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m_____________________________ test_compareWithCC ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compareWithCC\u001b[39;49;00m(supply_AA_BB_CC):\u001b[90m\u001b[39;49;00m\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m2\u001b[39;49;00m]==zz, \u001b[33m\"\u001b[39;49;00m\u001b[33maa and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: aa and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 45 == 35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixture.py\u001b[0m:20: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_fixture.py::\u001b[1mtest_compareWithAA\u001b[0m - AssertionError: aa and zz comparison failed\n",
      "\u001b[31mFAILED\u001b[0m test_fixture.py::\u001b[1mtest_compareWithCC\u001b[0m - AssertionError: aa and zz comparison failed\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.39s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_fixture.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c8f0c25",
   "metadata": {},
   "source": [
    "%%writefile test_parameterize.py\n",
    "import pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29533695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_parameterize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_parameterize.py\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90ab76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pytest.mark.parametrize(\"input1\",\"input2\",\"output\", [(5,5,10), (3,5,12)])\n",
    "def test_add(input1, input2, output):\n",
    "    assert input1+input2==output, \"test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3acede6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.0.0\n",
      "rootdir: C:\\Users\\pawne\\lab\n",
      "plugins: anyio-3.5.0\n",
      "collected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.03s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_parameterize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9caff4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
